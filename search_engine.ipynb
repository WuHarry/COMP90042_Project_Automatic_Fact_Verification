{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "import sys, os, lucene, time, threading, unicodedata, re, codecs\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from java.nio.file import Paths\n",
    "from org.apache.lucene.analysis.miscellaneous import LimitTokenCountAnalyzer\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.document import Document, Field, FieldType\n",
    "from org.apache.lucene.index import FieldInfo, IndexWriter, IndexWriterConfig, IndexOptions\n",
    "from org.apache.lucene.store import SimpleFSDirectory\n",
    "\n",
    "# Searching\n",
    "from org.apache.lucene.index import DirectoryReader\n",
    "from org.apache.pylucene.queryparser.classic import PythonMultiFieldQueryParser\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from org.apache.lucene.search import IndexSearcher, BooleanClause\n",
    "\n",
    "class Ticker(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tick = True\n",
    "\n",
    "    def run(self):\n",
    "        while self.tick:\n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(1.0)\n",
    "\n",
    "            \n",
    "class IndexFiles(object):\n",
    "    \"\"\"Usage: python IndexFiles <doc_directory>\"\"\"\n",
    "\n",
    "    def __init__(self, root, storeDir, doIndex=False):\n",
    "\n",
    "        self.analyzer = StandardAnalyzer() \n",
    "        \n",
    "        if not os.path.exists(storeDir):\n",
    "            os.mkdir(storeDir)\n",
    "        \n",
    "        if doIndex:\n",
    "            store = SimpleFSDirectory(Paths.get(storeDir))\n",
    "\n",
    "            analyzer = LimitTokenCountAnalyzer(self.analyzer, 1048576)\n",
    "            config = IndexWriterConfig(analyzer)\n",
    "            config.setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n",
    "            writer = IndexWriter(store, config)\n",
    "\n",
    "            self.indexDocs(root, writer)\n",
    "            ticker = Ticker()\n",
    "            print(\"commit index\")\n",
    "            threading.Thread(target=ticker.run).start()\n",
    "            writer.commit()\n",
    "            writer.close()\n",
    "            ticker.tick = False\n",
    "            print(\"done\")\n",
    "        \n",
    "        directory = SimpleFSDirectory(Paths.get(storeDir))\n",
    "        self.searcher = IndexSearcher(DirectoryReader.open(directory))\n",
    "\n",
    "    def indexDocs(self, root, writer):\n",
    "\n",
    "        t1 = FieldType()\n",
    "        t1.setStored(True)\n",
    "        t1.setTokenized(True)\n",
    "        t1.setIndexOptions(IndexOptions.DOCS_AND_FREQS)\n",
    "        \n",
    "        wikiFile = ZipFile(root, 'r')\n",
    "        files = wikiFile.namelist()\n",
    "        \n",
    "        i = 0\n",
    "        for file in files[1:]:\n",
    "            i += 1\n",
    "            wiki = wikiFile.open(file,'r')\n",
    "            for line in wiki:\n",
    "                for line in codecs.iterdecode(wiki, 'utf8'):\n",
    "                    normailized = unicodedata.normalize('NFD', line).split(' ', 2)\n",
    "                    if not normailized[1].isdigit(): continue\n",
    "                    docname = normailized[0] + ' ' + normailized[1]\n",
    "                    name = re.sub(r'[^a-zA-Z0-9]', ' ', normailized[0])\n",
    "                    contents = normailized[2]\n",
    "                    doc = Document()\n",
    "                    doc.add(Field('docname', docname, t1))\n",
    "                    doc.add(Field('name', name, t1))\n",
    "                    doc.add(Field('contents', contents, t1))\n",
    "                    writer.addDocument(doc)\n",
    "            print('File %d done indexing' % i, file)\n",
    "        \n",
    "    def searchDocs(self, command):\n",
    "\n",
    "        if command == '':\n",
    "            return\n",
    "\n",
    "#         print(\"Searching for:\", command)\n",
    "\n",
    "        parser = PythonMultiFieldQueryParser(['name', 'contents'], self.analyzer)\n",
    "        \n",
    "        query = parser.parse(command, ['name', 'contents'], \n",
    "                             [BooleanClause.Occur.SHOULD, BooleanClause.Occur.SHOULD], self.analyzer)\n",
    "        \n",
    "        scoreDocs = self.searcher.search(query, 20).scoreDocs\n",
    "#         print(\"%s total matching documents.\" % len(scoreDocs))\n",
    "\n",
    "        docName = []\n",
    "        docContents = []\n",
    "        \n",
    "        for scoreDoc in scoreDocs:\n",
    "            doc = self.searcher.doc(scoreDoc.doc)\n",
    "            docName.append(doc.get(\"docname\"))\n",
    "            docContents.append(doc.get(\"contents\"))\n",
    "            \n",
    "#             print('docname:', doc.get(\"docname\"), 'name:', doc.get(\"name\"), 'content:', doc.get(\"contents\"))\n",
    "        \n",
    "        return docName, docContents\n",
    "        \n",
    "    def retrieve(self, doc, sentenseid):\n",
    "\n",
    "        query = QueryParser.escape(doc + ' ' + str(sentenseid))\n",
    "        query = QueryParser('docname', self.analyzer).parse(query)\n",
    "        score = self.searcher.search(query, 1).scoreDocs\n",
    "\n",
    "        doc = self.searcher.doc(score[0].doc)\n",
    "        return doc.get('docname'), doc.get('contents')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
