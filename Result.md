**** Result

* First 0.7 Second Simple 0/1

* Big Set
('Label Accuracy', '\t\t48.02%')
('Sentence Precision', '\t15.73%')
('Sentence Recall', '\t35.56%')
('Sentence F1', '\t\t21.81%')
('Document Precision', '\t29.87%')
('Document Recall', '\t46.86%')
('Document F1', '\t\t36.49%')

* Small Set
Label Accuracy 		    48.09%
Sentence Precision 	    15.74%
Sentence Recall 	    35.74%
Sentence F1 		    21.86%
Document Precision 	    30.16%
Document Recall 	    47.27%
Document F1 		    36.83%
Support Label Accuracy 	58.91%
Refute Label Accuracy 	34.31%
NotInfo Label Accuracy 	51.05%


* First 0.9 Second Simple 0/1
Label Accuracy 		46.59%
Sentence Precision 	15.87%
Sentence Recall 	28.64%
Sentence F1 		20.42%
Document Precision 	30.68%
Document Recall 	38.29%
Document F1 		34.06%
Support Label Accuracy 	47.27%
Refute Label Accuracy 	29.15%
NotInfo Label Accuracy 	63.35%

* First 0/1 Second 0/1 Remove irrelevant evidence that input to second model
Label Accuracy 		49.51%
Sentence Precision 	34.19%
Sentence Recall 	32.81%
Sentence F1 		33.48%
Document Precision 	56.55%
Document Recall 	48.16%
Document F1 		52.02%
Support Label Accuracy 	62.69%
Refute Label Accuracy 	41.99%
NotInfo Label Accuracy 	43.85%

* First <0.8 Second 0/1 Remove irrelevant evidence that input to second model
Label Accuracy 		47.63%
Sentence Precision 	36.49%
Sentence Recall 	26.07%
Sentence F1 		30.41%
Document Precision 	59.34%
Document Recall 	39.19%
Document F1 		47.20%
Support Label Accuracy 	51.35%
Refute Label Accuracy 	35.51%
NotInfo Label Accuracy 	56.03%

* First <0.9 Second 0/1 Remove irrelevant evidence that input to second model
Label Accuracy 		46.75%
Sentence Precision 	37.37%
Sentence Recall 	22.12%
Sentence F1 		27.79%
Document Precision 	60.94%
Document Recall 	34.24%
Document F1 		43.84%
Support Label Accuracy 	44.75%
Refute Label Accuracy 	32.15%
NotInfo Label Accuracy 	63.35%

* First <0.7 Second 0/1 Remove irrelevant evidence that input to second model
Label Accuracy 		48.35%
Sentence Precision 	35.70%
Sentence Recall 	28.66%
Sentence F1 		31.79%
Document Precision 	58.43%
Document Recall 	42.87%
Document F1 		49.45%
Support Label Accuracy 	55.85%
Refute Label Accuracy 	38.15%
NotInfo Label Accuracy 	51.05%

* First <0.6 Second 0/1 Remove irrelevant evidence that input to second model
Label Accuracy 		49.13%
Sentence Precision 	34.82%
Sentence Recall 	30.91%
Sentence F1 		32.75%
Document Precision 	57.30%
Document Recall 	45.74%
Document F1 		50.88%
Support Label Accuracy 	60.05%
Refute Label Accuracy 	40.43%
NotInfo Label Accuracy 	46.91%

* First <0.5 Second 0/1 Remove irrelevant evidence that input to second model
Label Accuracy 		49.51%
Sentence Precision 	34.19%
Sentence Recall 	32.81%
Sentence F1 		33.48%
Document Precision 	56.55%
Document Recall 	48.16%
Document F1 		52.02%
Support Label Accuracy 	62.69%
Refute Label Accuracy 	41.99%
NotInfo Label Accuracy 	43.85%

